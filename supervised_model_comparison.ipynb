{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c594540",
   "metadata": {},
   "source": [
    "\n",
    "# Supervised Learning Model Comparison\n",
    "\n",
    "## Objective:\n",
    "We will compare three different supervised learning models using the **Forest Cover Type** dataset.\n",
    "The models will be evaluated based on **accuracy, training time, and confusion matrices**.\n",
    "\n",
    "## Models Compared:\n",
    "1. **Logistic Regression** - A simple, interpretable baseline model.\n",
    "2. **Gradient Boosting Classifier** - A powerful ensemble model that captures complex relationships.\n",
    "3. **Support Vector Machine (SVM)** - Works well in high-dimensional spaces.\n",
    "\n",
    "## Evaluation Criteria:\n",
    "- **Accuracy Score**: How well the model classifies data.\n",
    "- **Training Time**: How long the model takes to train.\n",
    "- **Confusion Matrix**: Visualization of classification performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be111e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47bb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"forest_cover_type_.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0469bdc",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing\n",
    "- We will separate features (X) and the target variable (y).\n",
    "- The dataset will be split into **80% training** and **20% testing**.\n",
    "- Features will be scaled using **StandardScaler** to improve model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f988cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['Cover_Type'])\n",
    "y = df['Cover_Type']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34463c65",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training and Evaluation\n",
    "We will train three models and evaluate them on:\n",
    "- **Accuracy Score**\n",
    "- **Training Time**\n",
    "- **Confusion Matrices**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(kernel='linear', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Training Time (s)\": train_time,\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}, Training Time: {train_time:.2f}s\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acfe51",
   "metadata": {},
   "source": [
    "\n",
    "## Confusion Matrices\n",
    "The confusion matrix helps us visualize misclassifications for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09223e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot confusion matrices\n",
    "for name, metrics in results.items():\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(metrics['Confusion Matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234033f",
   "metadata": {},
   "source": [
    "\n",
    "## Results Summary\n",
    "\n",
    "**Which method did you like the most?**  \n",
    "Gradient Boosting performed best in terms of accuracy while being efficient.\n",
    "\n",
    "**Which method did you like the least?**  \n",
    "Logistic Regression was the weakest performer due to the complexity of the dataset.\n",
    "\n",
    "**How did you score these supervised models?**  \n",
    "Accuracy and confusion matrices were used to compare performance.\n",
    "\n",
    "**Did the output align with your geologic understanding?**  \n",
    "Yes, as expected, some features were more predictive than others.\n",
    "\n",
    "**Did you hyperparameter tune? Why or why not?**  \n",
    "Not extensively, but default parameters were used to establish a baseline.\n",
    "\n",
    "**How did you split your data? Why does that make sense?**  \n",
    "80/20 split for training and testing, ensuring enough data for model learning.\n",
    "\n",
    "**What did you want to learn more about?**  \n",
    "More feature importance analysis and deeper hyperparameter tuning.\n",
    "\n",
    "**Did you pre-process your data?**  \n",
    "Yes, scaling was applied to numerical features.\n",
    "\n",
    "**Do all models require pre-processing?**  \n",
    "No, tree-based models (like Gradient Boosting) do not require feature scaling.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}